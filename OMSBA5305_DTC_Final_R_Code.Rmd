---
title: "OMSBA5305 Data Translation Challenge, R Code"
output:
  pdf_document: default
  html_notebook: default
---

# Load Libraries
```{r}
library(readxl)
library(dynlm) # use the dynlm package
library(urca) #use urca package, allows choosing drift and/or trend for ADF test
library(ggplot2)
library(stats)
library(lmtest) # For Z Statistic
library(forecast) 

```

#Import data and declare time series
```{r}
data<- read_excel("ch3_UnemploymentRate.xls")

#creating time series
unrate <- ts(data$UNRATE, frequency = 12, start = 1948)
data_length <- length(unrate)
unrate_forecast <- ts(data$UNRATE, frequency = 12, start=c(1948,1), end = c(2023,2))

```

## Initial Data Plot 
```{r}
ggplot() + 
  geom_line(data, mapping=aes(x=observation_date, y=UNRATE))+
  labs(title="GDP Forecast", colour="Forecasts") +
  xlab("Date") +
  ylab("GDP MoM% Rate")
```

# Step 1
## Remove Trend and Seasonality
```{r}
#removing trend and seasonality
unrate_d12 <- diff(unrate, differences = 12)
plot(unrate_d12)
```

## Create ACF & PACF
```{r}
acf(unrate_d12)
pacf(unrate_d12)

acf(unrate)
pacf(unrate)
```

## Create Linear Models
### Linear MA Model
```{r}
MA <- arima(unrate, order = c(0, 0, 1))
ma_r_squared <- as.numeric(MA[2]$sigma2[1])
ma_summary <- summary(MA)
# Calculate the adjusted R-squared value
ma_adjusted_r_squared <- 1 - (1 - ma_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value

print(paste0("MA Adjusted R-squared: ",ma_adjusted_r_squared))

# Z Test
ma_t_stat <- coeftest(MA)
# Print the t-statistics
print(ma_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis.

```

### Linear AR Models
#### AR(1)
```{r}
AR1 <- arima(unrate, order = c(1, 0, 0))
ar1_r_squared <- as.numeric(AR1[2]$sigma2[1])
ar1_summary <- summary(AR1)
# Calculate the adjusted R-squared value
ar1_adjusted_r_squared <- 1 - (1 - ar1_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value

print(paste0("AR(1) Adjusted R-squared: ",ar1_adjusted_r_squared))

# Z Test
ar1_t_stat <- coeftest(AR1)
# Print the t-statistics
print(ar1_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis.

```


#### AR(2)
```{r}
AR2 <- arima(unrate, order = c(2, 0, 0))
ar2_r_squared <- as.numeric(AR2[2]$sigma2[1])
ar2_summary <- summary(AR2)
# Calculate the adjusted R-squared value
ar2_adjusted_r_squared <- 1 - (1 - ar2_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value
print(paste0("AR(2) adjusted R-squared: ", ar2_adjusted_r_squared))

# Z Test
ar2_t_stat <- coeftest(AR2)
# Print the t-statistics
print(ar2_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis.
```
#### AR(3)
```{r}
AR3 <- arima(unrate, order = c(3, 0, 0))
ar3_r_squared <- as.numeric(AR3[2]$sigma2[1])
ar3_summary <- summary(AR3)
# Calculate the adjusted R-squared value
ar3_adjusted_r_squared <- 1 - (1 - ar3_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value

print(paste0("AR(3) Adjusted R-squared: ",ar3_adjusted_r_squared))

# Z Test
ar3_t_stat <- coeftest(AR3)
# Print the t-statistics
print(ar3_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis.
```


### Linear ARMA Model 1
```{r}
ARMA1 <- arima(unrate, order = c(1, 0, 1))
arma1_r_squared <- as.numeric(ARMA1[2]$sigma2[1])
arma1_summary <- summary(ARMA1)

# Calculate the adjusted R-squared value
arma1_adjusted_r_squared <- 1 - (1 - arma1_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value
print(paste0("ARMA Model 1 Adjusted R-squared: ",arma1_adjusted_r_squared))

# Z Test
arma1_t_stat <- coeftest(ARMA1)
# Print the t-statistics
print(arma1_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis. In support of AR model. 
```
### Linear ARMA Model 2
```{r}
ARMA2 <- arima(unrate, order = c(2, 0, 1))
arma2_r_squared <- as.numeric(ARMA2[2]$sigma2[1])
arma2_summary <- summary(ARMA2)

# Calculate the adjusted R-squared value
arma2_adjusted_r_squared <- 1 - (1 - arma2_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value
print(paste0("ARMA Model 2 Adjusted R-squared: ",arma2_adjusted_r_squared))

# Z Test
arma2_t_stat <- coeftest(ARMA2)
# Print the t-statistics
print(arma2_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis. In support of AR model. 
```

### Linear ARMA Model 3
```{r}
ARMA3 <- arima(unrate, order = c(1, 0, 4))
arma3_r_squared <- as.numeric(ARMA3[2]$sigma2[1])
arma3_summary <- summary(ARMA3)

# Calculate the adjusted R-squared value
arma3_adjusted_r_squared <- 1 - (1 - arma3_r_squared) * (data_length - 1) / (data_length)
# Print the adjusted R-squared value
print(paste0("ARMA Model 3 Adjusted R-squared: ",arma3_adjusted_r_squared))

# Z Test
arma3_t_stat <- coeftest(ARMA3)
# Print the t-statistics
print(arma3_t_stat) ### High Z statistics means there is a strong correlation, P value is small so we can reject null hypothesis. In support of AR model. 
```

## Check for stationary 
Test statistic: The Dickey-Fuller test produces a test statistic that measures the strength of evidence against the null hypothesis of non-stationarity. It is denoted as "t-value" or "adf" in the test results. The more negative (or less positive) the test statistic, the stronger the evidence for rejecting the null hypothesis.
```{r}
# Linear MA Model
adf_MA <- ur.df(MA$residuals)
print(adf_MA)

# Linear AR Model
adf_AR1 <- ur.df(AR1$residuals)
print(adf_AR1)

adf_AR2 <- ur.df(AR2$residuals)
print(adf_AR2)

adf_AR3 <- ur.df(AR3$residuals)
print(adf_AR3)

# Linear ARMA Model
adf_ARMA1 <- ur.df(ARMA1$residuals)
print(adf_ARMA1)

adf_ARMA2 <- ur.df(ARMA2$residuals)
print(adf_ARMA2)

adf_ARMA3 <- ur.df(ARMA3$residuals)
print(adf_ARMA3)

```

##  AIC
AIC stands for Akaike Information Criterion. It is a measure used in statistics to assess the relative quality of statistical models. The AIC value is calculated based on the likelihood function of the model and the number of parameters estimated in the model.

The AIC provides a balance between model fit and model complexity. It penalizes models that have a large number of parameters, favoring simpler models that explain the data well. The goal is to find a model that minimizes the AIC value.

The AIC value is calculated using the following formula:

AIC = -2 * log-likelihood + 2 * k

where:

log-likelihood is the logarithm of the maximum likelihood estimate of the model.
k is the number of parameters estimated in the model.
In general, lower AIC values indicate better-fitting models, as they strike a balance between goodness of fit and model complexity. When comparing multiple models, the model with the lowest AIC value is typically considered the best-fitting model.

It's worth noting that the AIC is a relative measure, meaning that it is useful for comparing different models but does not provide an absolute assessment of model fit or prediction accuracy. Other information criteria, such as the Bayesian Information Criterion (BIC), can also be used for model selection in a similar manner.
```{r}
AIC(MA)
AIC(AR1)
AIC(AR2)
AIC(AR3)


AIC(ARMA1)
AIC(ARMA2)
AIC(ARMA3)
```

## BIC 
BIC stands for Bayesian Information Criterion. It is another criterion used for model selection in statistics. Like the AIC (Akaike Information Criterion), the BIC provides a measure of the relative quality of statistical models.

The BIC takes into account both the goodness of fit of the model and the complexity of the model, similar to the AIC. However, the BIC places a higher penalty on model complexity than the AIC. It is derived from a Bayesian perspective, assuming a uniform prior distribution over the model parameters.

The BIC value is calculated using the following formula:

BIC = -2 * log-likelihood + log(n) * k

where:

log-likelihood is the logarithm of the maximum likelihood estimate of the model.
n is the number of observations in the data.
k is the number of parameters estimated in the model.
The BIC penalizes models with more parameters more heavily than the AIC, effectively favoring simpler models. Therefore, the BIC tends to produce a more parsimonious model compared to the AIC.

Similar to the AIC, lower BIC values indicate better-fitting models. When comparing multiple models, the model with the lowest BIC value is typically considered the best-fitting model.

It's important to note that the AIC and BIC are both useful criteria for model selection, and the choice between them depends on the specific context and preferences. In some cases, the AIC may favor more complex models, while the BIC tends to favor simpler models.

```{r}
print(paste0("MA BIC: ", BIC(MA)))
print(paste0("AR1 BIC: ", BIC(AR1)))
print(paste0("AR2 BIC: ", BIC(AR2)))
print(paste0("AR3 BIC: ", BIC(AR3)))
print(paste0("ARMA 1 BIC: ", BIC(ARMA1)))
print(paste0("ARMA 2 BIC: ", BIC(ARMA2)))
print(paste0("ARMA 3 BIC: ", BIC(ARMA3)))
```


## Ljung Box Test
The test is based on the Ljung-Box statistic, which measures the overall autocorrelation of the residuals at different lags. The null hypothesis of the test is that there is no autocorrelation in the residuals.
```{r}
Box.test(unrate, type='Ljung-Box')
```

#Step 2:
##Import data and declare time series
Declaration: An Excel file named "ch3_UnemploymentRate.xls" is read using read_excel(). The data is then divided into the first 90% portion (data_first_90pct) and the estimation sample (data_estimation_sample). A time series object unrate_forecast is created based on the "UNRATE" column.

```{r}
# create train data, first 90%
data_row_ct <- nrow(data)
data_first_90pct <- data[1:ceiling(0.9*data_row_ct),]
data_first_90pct_row_ct <- nrow(data_first_90pct)
# set test data
data_estimation_sample <- tail(data, data_row_ct-ceiling(0.9*data_row_ct))
data_estimation_sample_row_ct <- data_row_ct-ceiling(0.9*data_row_ct)

```

## Model 1: AR(3) (Fixed Scheme): 
An AR(3) model is fitted to the estimation sample using dynlm(). The model is summarized, and a for loop is used to generate forecasts (fcast1) and forecast errors (ferror1) based on the model coefficients. The mean squared error (MSE) is calculated (MSE1). The model is then tested for forecast optimality using the MPE test and the informational efficiency test.

### 90% Control, 10% Test
```{r}
#fit AR(3)
ar3_model<-dynlm(unrate_forecast ~ lag(unrate_forecast,-1) + lag(unrate_forecast,-3), start=c(1948,1), end=c(2023,2))
summary(ar3_model)


fcast1<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
ferror1<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
loss1<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)

for (i in 1:data_estimation_sample_row_ct){ #start a for loop 
  #fill in forecasted values at the end of each iteration
  ## Forecast values
  fcast1[i]<-coef(ar3_model)[1]+ coef(ar3_model)[2]*unrate_forecast[data_first_90pct_row_ct-1+i] + coef(ar3_model)[3]*unrate_forecast[data_first_90pct_row_ct-3+i]
  ## Error Values
  ferror1[i]<-unrate_forecast[data_first_90pct_row_ct+i]- fcast1[i]
  ## Loss Values
  loss1[i] <-ferror1[i]^2
} #end the loop

# cbind(fcast1, ferror1, loss1)
MSE1 <- mean(loss1)

mpetest <- lm(ferror1 ~ 1)
summary(mpetest)

IETest <- lm(ferror1 ~ fcast1)
summary(IETest)

```

### AR(3) One Step Ahead
```{r}
ar3_onestep <- rbind(NA,NA,NA, data.frame(predict(ar3_model, h=1)))

ar3_data_onestep <- cbind(data, ar3_onestep)
colnames(ar3_data_onestep)[3] <- "forecast"

ar3_onestep_error <- ar3_data_onestep$UNRATE - ar3_data_onestep$forecast
ar3_onestep_loss <- (ar3_data_onestep$UNRATE - ar3_data_onestep$forecast)^2
ar3_onestep_mse <- mean((ar3_data_onestep$UNRATE - ar3_data_onestep$forecast)^2)

fAR3_onestep <- cbind(data, ar3_data_onestep[3], ar3_onestep_error, ar3_onestep_loss)
fcast1_onestep <- fAR3_onestep$forecast
ar3_onestep_mpetest <- lm(ferror1 ~ 1)
summary(ar3_onestep_mpetest)

ar3_onestep_IETest <- lm(ar3_onestep_error ~ fAR3_onestep$forecast)
summary(ar3_onestep_IETest)
```


## Model 2: Naive Forecast: 
A naive forecast is generated by assigning the previous observation as the forecast value (fcast2). Forecast errors and MSE are calculated, and the model is tested for forecast optimality.
```{r}
fcast2<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
ferror2<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
loss2<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)

for (i in 1:data_estimation_sample_row_ct){ 
  fcast2[i]<-unrate_forecast[data_first_90pct_row_ct-1+i] #naive forecast
  ferror2[i]<-unrate_forecast[data_first_90pct_row_ct+i]- fcast2[i] #fill in forecasted values at the end of each iteration
  loss2[i] <-ferror2[i]^2
 }
cbind(fcast2, ferror2, loss2)
MSE2 <- mean(loss2)

mpetest <- lm(ferror2 ~ 1)
summary(mpetest)


IETest <- lm(ferror2 ~ fcast2)
summary(IETest)

```


#Model 3: Average-4 Forecast: 
The forecast is calculated as the average of the last four observations (fcast3). Forecast errors and MSE are calculated, and the model is tested for forecast optimality.
```{r}
fcast3<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
ferror3<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
loss3<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)


for (i in 1:data_estimation_sample_row_ct){ 
  fcast3[i]<-(unrate_forecast[data_estimation_sample_row_ct-1+i]+unrate_forecast[data_estimation_sample_row_ct-2+i]+unrate_forecast[data_estimation_sample_row_ct-3+i]+unrate_forecast[data_estimation_sample_row_ct-4+i])/4  #forecast is the average of the last 4 observations
  ferror3[i]<-unrate_forecast[data_first_90pct_row_ct+i] - fcast3[i] #fill in forecasted values at the end of each iteration
  loss3[i] <-ferror3[i]^2
}


cbind(fcast3, ferror3, loss3)
MSE3 <- mean(loss3)

mpetest <- lm(ferror3 ~ 1)
summary(mpetest)


IETest <- lm(ferror3 ~ fcast3)
summary(IETest)

```

## Optimal Linear Combinations: 
An optimal linear combination of the three forecasts is created using the lm() function (comb). The fitted values of the combination model are used as the forecast values (fcast4). Forecast errors and MSE are calculated for this combined forecast (ferror4, loss4, and MSE4).
```{r}

unrate_forecast0<-window(unrate_forecast,start=c(2015,9)) ## estimation sample date
comb<-lm(unrate_forecast0~fcast1+fcast2+fcast3)
summary(comb)


fcast4<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
ferror4<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)
loss4<-numeric(data_estimation_sample_row_ct) #generate a vector of zeros (length of estimation sample set)

fcast4 <- comb$fitted.values
ferror4 <- unrate_forecast0-fcast4
loss4 <-ferror4^2

MSE4 <- mean(loss4)
```

# Forecasts Findings Overview
```{r}
data_estimation_sample
cbind(data_estimation_sample, fcast1, ferror1, loss1, fcast2, ferror2, loss2, fcast3, ferror3, loss3)
```

# Plots
```{r}
forec <- cbind(data_estimation_sample,fcast1, fcast1_onestep,fcast2, fcast3, fcast4)
## Adjust date tye
forec$observation_date = as.Date(forec$observation_date)
data$observation_date = as.Date(data$observation_date)
forec
```
## Full Timeseries Plot + Forecast
```{r}
plot(x = data$observation_date, y = data$UNRATE, type = "l", lwd=3.0)+
  lines(x = forec$observation_date, y= forec$fcast1,lty=2,lwd=2,col="green")+
  lines(x = forec$observation_date, y= forec$fcast2,lty=2,lwd=2,col="red")+
  lines(x = forec$observation_date, y= forec$fcast3,lty=2,lwd=2,col="orange")
# +
#   lines(x = forec$observation_date, y= forec$fcast4,lty=2,lwd=2,col="purple")

```

## Estimation Sample Plot
```{r}

plot(x = data$observation_date, y = data$UNRATE, type = "l", lwd=5.0, xlim=c(as.Date("2015-01-01"), as.Date("2023-01-04")) )+
  lines(x = forec$observation_date, y= forec$fcast1,lty=2,lwd=2,col="green")+
  lines(x = forec$observation_date, y= forec$fcast2,lty=2,lwd=2,col="red")+
  lines(x = forec$observation_date, y= forec$fcast3,lty=2,lwd=2,col="orange")
# +
#   lines(x = forec$observation_date, y= forec$fcast4,lty=2,lwd=2,col="purple")

```



```{r}
plot(x = data$observation_date, y = data$UNRATE, type = "l", lwd=5.0, xlim=c(as.Date("2020-01-01"), as.Date("2023-01-04")) )+
  lines(x = forec$observation_date, y= forec$fcast1,lty=2,lwd=2,col="green")+
  lines(x = forec$observation_date, y= forec$fcast2,lty=2,lwd=2,col="red")+
  lines(x = forec$observation_date, y= forec$fcast3,lty=2,lwd=2,col="orange")
# +
#   lines(x = forec$observation_date, y= forec$fcast4,lty=2,lwd=2,col="purple")

```

```{r}
plot(x = data$observation_date, y = data$UNRATE, type = "l", lwd=5.0, xlim=c(as.Date("2021-01-01"), as.Date("2023-01-04")) )+
  lines(x = forec$observation_date, y= forec$fcast1,lty=2,lwd=2,col="green")+
  lines(x = forec$observation_date, y= forec$fcast2,lty=2,lwd=2,col="red")+
  lines(x = forec$observation_date, y= forec$fcast3,lty=2,lwd=2,col="orange")

```

## Forcasted Data Plot
```{r}
ggplot() + 
  geom_line(data, mapping=aes(x=observation_date, y=UNRATE),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast1, color= 'Model 1: Fixed Scheme'),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast2, color='Model 2: Naive'),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast3, color='Model 3: Average-4'),size=1) +
  #geom_line(forec, mapping=aes(x=observation_date, y=fcast4, color='fcast4'),size=1) +
  xlim(as.Date(c("2015-01-01", "2023-01-01"))) +
  labs(title="GDP Forecast", colour="Forecasts") +
  xlab("Date") +
  ylab("GDP MoM% Rate")
```

## Forcasted Data Plot
```{r}
ggplot() + 
  geom_line(data, mapping=aes(x=observation_date, y=UNRATE),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast1, color= 'Model 1: Fixed Scheme'),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast2, color='Model 2: Naive'),size=1) +
  geom_line(forec, mapping=aes(x=observation_date, y=fcast3, color='Model 3: Average-4'),size=1) +
  #geom_line(forec, mapping=aes(x=observation_date, y=fcast4, color='fcast4'),size=1) +
  xlim(as.Date(c("1948-01-01", "2023-01-01"))) +
  labs(title="GDP Forecast", colour="Forecasts") +
  xlab("Date") +
  ylab("GDP MoM% Rate")+
  theme(aspect.ratio=1/2)
```

```{r}
ggplot() + 
  # geom_line(data, mapping=aes(x=observation_date, y=UNRATE)) +
  geom_line(fAR3_onestep, mapping = aes(x = observation_date, y = UNRATE))+
  geom_line(fAR3_onestep, mapping = aes(x = observation_date, y = forecast, color= 'yellow')))

fAR3_onestep
```




# Part 3